# Heartbeat Monitor: Agente de Monitorizaci√≥n de Alto Rendimiento

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.14-blue.svg" alt="Python 3.14">
  <img src="https://img.shields.io/badge/FastAPI-0.109-009688.svg" alt="FastAPI">
  <img src="https://img.shields.io/badge/AlpineJS-3.x-8bc34a.svg" alt="AlpineJS">
  <img src="https://img.shields.io/badge/Docker-passing-brightgreen.svg" alt="Docker Build Status">
</p>

Un agente de monitorizaci√≥n ligero, modular y concurrente dise√±ado espec√≠ficamente para entornos Dockerizados. Este sistema no solo verifica la disponibilidad, sino que optimiza la latencia de red y gestiona el estado de los servicios con una arquitectura resiliente.

Desarrollado en **Python 3.14 (Alpine)**, enfocado en la eficiencia de recursos y la precisi√≥n de m√©tricas.

## üìä Dashboard de Anal√≠tica

El sistema incluye un panel de control moderno para visualizar la salud de tu infraestructura.

* **Frontend:** Construido con **AlpineJS** y **ECharts**. Ligero, sin build-step complejo, con actualizaciones en tiempo real ("Live Mode") y visualizaci√≥n de **Jitter**.
* **Backend:** API RESTful de alto rendimiento con **FastAPI**. Implementa **Resoluci√≥n Din√°mica** (`TARGET_DATA_POINTS = 30`) para garantizar gr√°ficos fluidos sin importar el rango de tiempo consultado (desde 5 minutos hasta 30 d√≠as).

## üèóÔ∏è Arquitectura del Sistema

El sistema utiliza un patr√≥n de **Productor-Consumidor desacoplado** a trav√©s de la base de datos compartida.

```ascii
+----------------------+           +------------------------+
|   HEARTBEAT AGENT    |  (Write)  |     SQLITE (WAL)       |
| (Python / Productor) |---------->| (Persistencia H√≠brida) |
+----------------------+           +------------------------+
          ^                                    ^
          | (10s Loop)                         |
          |                                    | (Read-Only :ro)
+---------+------------+           +-----------+------------+
| Servicios / Docker   |           |   DASHBOARD BACKEND    |
| (Target a Monitorear)|           | (FastAPI / Consumidor) |
+----------------------+           +-----------+------------+
                                               ^
                                               | (JSON / REST)
                                               v
                                   +------------------------+
                                   |   DASHBOARD FRONTEND   |
                                   |   (AlpineJS / ECharts) |
                                   +------------------------+
```

1. **Agente (Escritura):** Tiene acceso exclusivo de escritura a la DB. Usa modo WAL para no bloquear lecturas.
2. **Dashboard (Lectura):** Monta el volumen de datos como `read-only` (`:ro`). Si el agente cae, el dashboard sigue mostrando datos hist√≥ricos.
3. **Frontend:** Consume la API del backend mediante *polling* inteligente (cada 2s en modo Live).

## üöÄ Caracter√≠sticas T√©cnicas Destacadas

M√°s que un simple script de "ping", este proyecto implementa patrones de ingenier√≠a para resolver problemas comunes en monitorizaci√≥n distribuida:

* **‚ö° Arquitectura Concurrente:** Implementaci√≥n de `ThreadPoolExecutor` para paralelizar operaciones de I/O (solicitudes HTTP, consultas a sockets Docker), desacoplando la recolecci√≥n de m√©tricas del bloqueo de red y garantizando ciclos de ejecuci√≥n precisos.
* **üß† Red Inteligente (Smart Networking):**
  * **DNS Override & Host Injection:** Mecanismo capaz de interceptar tr√°fico hacia servicios internos, resolviendo directamente a IPs locales e inyectando cabeceras `Host`. Esto elimina la latencia de resoluci√≥n DNS externa y el overhead de SSL en redes internas (reducci√≥n de ~50ms a ~2ms).
  * **IPv4 Enforcement:** Adaptadores HTTP personalizados a nivel de transporte para mitigar los retrasos de resoluci√≥n IPv6 comunes en contenedores Alpine Linux.
* **üê≥ Protocolo Docker Nativo:** Soporte para el esquema `docker:<container_name>`, permitiendo verificaciones de salud directas contra el socket Unix de Docker (`/var/run/docker.sock`) para servicios que no exponen puertos HTTP.
* **üõ°Ô∏è Resiliencia de Datos:** Uso de SQLite en modo **WAL (Write-Ahead Logging)** para permitir alta concurrencia en operaciones de lectura/escritura sin bloqueos de base de datos.
* **üîî Gesti√≥n de Estado con "Debounce":** Sistema de alertas inteligente que filtra falsos positivos mediante umbrales de cambio de estado configurables y l√≥gica de reintentos autom√°tica ante fallos del webhook.

## ‚öôÔ∏è Flujo de Ejecuci√≥n del Agente

El agente opera en un bucle principal, ejecut√°ndose cada 10 segundos, coordinando la recolecci√≥n, procesamiento y notificaci√≥n.

```ascii
[ INICIO ]
    |
    v
[ 1. Cargar Configuraci√≥n (.env) ]
    |
    v
[ 2. Init Base de Datos (SQLite WAL) ]
    |
    +---> [ BUCLE PRINCIPAL (Cada 10s) ] <--------------------------+
            |                                                       |
            |-- (A) M√©tricas Sistema (CPU/RAM) [S√≠ncrono]           |
            |                                                       |
            |-- (B) Health Checks [ThreadPoolExecutor / Paralelo]   |
            |       |--> HTTP/HTTPS (Smart Request)                 |
            |       |--> Docker Socket                              |
            |       +--> Ping Internet                              |
            |                                                       |
            v                                                       |
    [ 3. Procesar Estado (Debounce Logic) ]                         |
            |                                                       |
            +--- ¬øCambio de Estado? ---> [ Enviar Alerta (N8N) ]    |
            |                                                       |
            +--- ¬øInternet OK? --------> [ Enviar Heartbeat (CF) ]  |
            |                                                       |
            v                                                       |
    [ 4. Persistencia (Guardar M√©tricas en DB) ] -------------------+
```

### Flujo de Ejecuci√≥n Detallado (Ciclo de 10s)

1. **Inicializaci√≥n:** Carga de configuraci√≥n y establecimiento de conexiones persistentes (Keep-Alive).
2. **M√©tricas de Sistema (S√≠ncrono):** Lectura instant√°nea de CPU/RAM/Disco (`psutil`).
3. **Health Checks (Paralelo)::** Se lanzan hilos concurrentes para verificar todos los servicios configurados y la conectividad a Internet.
4. **Procesamiento de Estado:** Se eval√∫an los cambios (Healthy <-> Unhealthy) contra los umbrales definidos.
5. **Notificaci√≥n/Heartbeat:** Si hay cambios cr√≠ticos o corresponde un latido, se env√≠an payloads JSON optimizados a los endpoints externos.
6. **Persistencia:** Se realiza un commit at√≥mico de todas las m√©tricas del ciclo en la base de datos local.

## üìÇ Estructura del C√≥digo (Monorepo)

El proyecto ha evolucionado hacia una arquitectura de **Monorepo** para gestionar tanto el agente principal como las herramientas de visualizaci√≥n y desarrollo:

```text
/
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îú‚îÄ‚îÄ heartbeat/     # Agente de Monitorizaci√≥n (Python Service)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py        # Orquestador principal.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py      # Gesti√≥n de configuraci√≥n.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitors.py    # L√≥gica de health checks y m√©tricas.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alerts.py      # Gesti√≥n de estado y notificaciones.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ network.py     # Capa de red (Smart Request, IPv4).
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py    # Persistencia SQLite.
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/     # Panel de Visualizaci√≥n (Nuevo)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backend/       # API FastAPI para anal√≠tica.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ frontend/      # UI Reactiva (AlpineJS + ECharts).
‚îÇ   ‚îî‚îÄ‚îÄ mocks/         # Mock Server para desarrollo local
‚îÇ       ‚îú‚îÄ‚îÄ server.py      # Servidor Python de pruebas.
‚îÇ       ‚îî‚îÄ‚îÄ templates/     # UI del Mock Controller.
‚îú‚îÄ‚îÄ data/              # Vol√∫menes persistentes (DBs, logs)
‚îÇ   ‚îú‚îÄ‚îÄ metrics.db     # Base de datos Producci√≥n.
‚îÇ   ‚îú‚îÄ‚îÄ metrics_dev.db # Base de datos Desarrollo.
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ docker-compose.prod.yml  # Stack Producci√≥n (Agente + Dashboard).
‚îú‚îÄ‚îÄ docker-compose.dev.yml   # Stack Desarrollo (Agente + Dashboard + Mock).
‚îú‚îÄ‚îÄ .env.prod.example        # Plantilla env Producci√≥n.
‚îú‚îÄ‚îÄ .env.dev.example         # Plantilla env Desarrollo.
‚îî‚îÄ‚îÄ ...
```

### Descripci√≥n de M√≥dulos (Heartbeat Agent)

* **`main.py`**: Contiene el bucle principal de ejecuci√≥n de la aplicaci√≥n. Coordina la inicializaci√≥n, la recolecci√≥n de datos, el procesamiento de estado y la persistencia de m√©tricas.
* **`config.py`**: Centraliza la lectura de variables de entorno, la definici√≥n de constantes globales y el parseo de la configuraci√≥n de servicios a monitorizar.
* **`monitors.py`**: Agrupa las funciones responsables de obtener datos del sistema (CPU, RAM, Disco) y realizar los health checks HTTP/HTTPS y Docker.
* **`alerts.py`**: Implementa la l√≥gica de gesti√≥n de estado transitorio y estable, as√≠ como el mecanismo de env√≠o de alertas a trav√©s de webhooks N8N y la comunicaci√≥n de latidos al worker de Cloudflare.
* **`network.py`**: Provee la capa de abstracci√≥n para red, incluyendo optimizaci√≥n de sesiones y la l√≥gica `smart_request` para DNS Override.
* **`database.py`**: Encapsula todas las operaciones relacionadas con la base de datos SQLite y el guardado de m√©tricas recolectadas en cada ciclo.

## Monitorizaci√≥n de Servicios

La funcionalidad principal del agente es monitorizar el estado de m√∫ltiples servicios web, reportarlo al worker y generar alertas si su estado cambia de forma persistente.

### Configuraci√≥n Din√°mica

Los servicios a monitorear se configuran din√°micamente mediante variables de entorno:

1. **`SERVICE_NAMES`**: Lista separada por comas de los nombres de los servicios (ej: `SERVICE_NAMES=nextjs,strapi,umami`).
2. **`SERVICE_URL_{nombre}`**: La URL a chequear para cada nombre definido (ej: `SERVICE_URL_nextjs=https://www.ejemplo.com`).

Un servicio se considera `"healthy"` si responde con un c√≥digo `2xx` o `3xx`. De lo contrario, se marca como `"unhealthy"`.

### Configuraci√≥n Avanzada de Servicios

El monitor soporta caracter√≠sticas avanzadas para cubrir casos de uso complejos, como servicios internos o endpoints protegidos.

#### 1. Monitoreo Directo de Contenedores (`docker:`)

Para servicios de infraestructura (como Nginx, t√∫neles, bases de datos) que no exponen un puerto HTTP accesible f√°cilmente, puedes usar el protocolo `docker:`. Esto verifica directamente si el contenedor est√° en estado `running`.

* **Sintaxis:** `SERVICE_URL_<nombre>="docker:<nombre_del_contenedor>"`
* **Ejemplo:**

    ```bash
    SERVICE_URL_nginx="docker:mi-contenedor-nginx"
    ```

* **Nota:** Requiere que el agente tenga acceso al socket de Docker (`/var/run/docker.sock`).

#### 2. Headers HTTP Personalizados

Algunos endpoints de salud requieren autenticaci√≥n o headers espec√≠ficos para responder correctamente. Puedes definirlos usando variables de entorno con el prefijo `SERVICE_HEADERS_`.

* **Sintaxis:** `SERVICE_HEADERS_<nombre>="Header1:Valor1,Header2:Valor2"`
* **Ejemplo:**

    ```bash
    # Verifica un endpoint que requiere un token o flag especial
    SERVICE_URL_api="https://mi-api.com/health"
    SERVICE_HEADERS_api="x-health-check:true,Authorization:Bearer mi-token"
    ```

### Optimizaci√≥n de Latencia (DNS Override)

Para entornos donde los servicios residen en la misma red local o servidor (ej: contenedores Docker detr√°s de un Nginx en el host), el agente permite configurar una IP de anulaci√≥n de DNS (`INTERNAL_DNS_OVERRIDE_IP`) para reducir dr√°sticamente la latencia.

* **Funcionamiento:** Si se define esta variable, el agente interceptar√° las peticiones a los servicios monitorizados, resolver√° el dominio directamente a la IP especificada, forzar√° el uso de HTTP (evitando el handshake SSL innecesario en la red interna) e inyectar√° el encabezado `Host` correcto.
* **Beneficio:** Reduce la latencia de ~50ms a ~1-3ms al saltarse la resoluci√≥n DNS externa y el enrutamiento p√∫blico.
* **Configuraci√≥n:** Ver variable `INTERNAL_DNS_OVERRIDE_IP` en `.env`.

### Payload de Estado de Salud

En cada ciclo, el agente construye un payload JSON que resume el estado de salud de los servicios y lo env√≠a al `HEARTBEAT_URL`.

* **Estructura del Payload:**

    ```json
    {
      "services": {
        "nextjs": { "status": "healthy" },
        "strapi": { "status": "unhealthy" },
        "umami": { "status": "healthy" }
      }
    }
    ```

## Gesti√≥n de Estado y Alertas

Para evitar falsas alarmas por fallos transitorios y gestionar las notificaciones de forma centralizada, el agente implementa una **arquitectura de estado unificada**.

Toda la l√≥gica de estado se gestiona a trav√©s de una √∫nica funci√≥n gen√©rica, `check_state_change`, y se almacena en un diccionario global en memoria, `global_states`. Este enfoque permite monitorear cualquier item (el worker principal o servicios individuales) usando las mismas reglas, evitando la duplicaci√≥n de c√≥digo.

### L√≥gica de Notificaci√≥n

El sistema env√≠a alertas al `N8N_WEBHOOK_URL` bajo las siguientes condiciones, incluyendo ahora mecanismos de robustez y detalle:

1. **Robustez y Reintentos:**
    * Si el env√≠o de la alerta falla (por ejemplo, timeout del webhook), el sistema reintenta autom√°ticamente hasta **3 veces** antes de desistir, asegurando que las alertas cr√≠ticas lleguen a su destino.

2. **Alertas Enriquecidas:**
    * **Servicio Ca√≠do:** Incluye la raz√≥n espec√≠fica del fallo (ej: `HTTP 500`, `Timeout`, `Container Exited`) para facilitar el diagn√≥stico inmediato.
    * **Servicio Recuperado:** Muestra la latencia actual del servicio tras la recuperaci√≥n.
    * **Timestamp:** Todas las alertas incluyen la fecha y hora exacta del evento (zona horaria configurada) para una auditor√≠a precisa.

3. **Condiciones de Disparo:**
    * **Ca√≠da de Servicio:** Tras `STATUS_CHANGE_THRESHOLD` fallos consecutivos.
    * **Recuperaci√≥n de Servicio:** Inmediata al primer √©xito.
    * **Estado del Worker:** Monitorizaci√≥n de cambios de estado del propio worker de Cloudflare con alertas contextuales.

Este mecanismo asegura que solo se notifiquen los cambios de estado confirmados, aplicando una l√≥gica consistente a todos los elementos monitoreados.

## üíæ Persistencia de Datos (Esquema Relacional)

El sistema utiliza **SQLite** en modo **WAL (Write-Ahead Logging)** para permitir escrituras de alta concurrencia desde el agente y lecturas simult√°neas desde el dashboard sin bloqueos. El esquema ha sido normalizado para soportar consultas anal√≠ticas eficientes.

### Tabla 1: `monitoring_cycles` (Hechos Globales)

Almacena una fila por cada ciclo de ejecuci√≥n (10s).

| Columna | Tipo | Descripci√≥n |
| :--- | :--- | :--- |
| `id` | `TEXT (PK)` | UUID √∫nico del ciclo. |
| `timestamp_lima`| `TEXT` | Marca de tiempo ISO8601 (Indexado). |
| `cpu_percent` | `REAL` | Uso de CPU global. |
| `ram_percent` | `REAL` | Uso de RAM global. |
| `disk_percent`| `REAL` | Uso de disco ra√≠z. |
| `uptime_seconds`| `REAL` | Uptime del sistema host. |
| `container_count`| `INTEGER`| Total de contenedores Docker corriendo. |
| `internet_status` | `BOOLEAN`| `1` (Online) / `0` (Offline). |
| `ping_ms` | `REAL` | Latencia a Internet (ICMP/HTTP Ping). |
| `worker_status` | `INTEGER` | C√≥digo de estado HTTP retornado por la API del Cloudflare Worker. Refleja el resultado del procesamiento del latido. <br> - `200`: **√âxito**. Latido recibido, procesado y el estado del host/servicios fue actualizado. Puede indicar un estado "recorded" (sin cambios) o "recovered" (recuperaci√≥n). <br> - `220`: **Advertencia (Ciego)**. Latido recibido y timestamp actualizado, pero la API no pudo leer el estado *anterior* de su base de datos. No se pudo determinar si hubo una recuperaci√≥n. <br> - `221`: **Advertencia (Fallo en Actualizaci√≥n de Recuperaci√≥n)**. Se detect√≥ una recuperaci√≥n, pero la API fall√≥ al actualizar su propio estado o al enviar la notificaci√≥n. <br> - `500`: **Error Cr√≠tico del Worker**. La API fall√≥ en un paso esencial (ej. escribir el timestamp inicial) y el latido fue abortado. <br> - `NULL`: **Error del Agente Local**. El script de monitorizaci√≥n no pudo contactar la API del worker (ej. timeout, error de red, DNS). |
| `cycle_duration_ms` | `INTEGER` | Tiempo total de ejecuci√≥n del ciclo. |

### Tabla 2: `service_checks` (Detalle por Servicio)

Almacena el estado individual de cada servicio monitoreado en un ciclo. Relaci√≥n 1:N con `monitoring_cycles`.

| Columna | Tipo | Descripci√≥n |
| :--- | :--- | :--- |
| `id` | `INTEGER (PK)` | Auto-incremental. |
| `cycle_id` | `TEXT (FK)` | Referencia a `monitoring_cycles.id`. |
| `service_name` | `TEXT` | Nombre del servicio (Indexado). |
| `service_url` | `TEXT` | Endpoint verificado. |
| `status` | `TEXT` | `'healthy'` o `'unhealthy'`. |
| `latency_ms` | `REAL` | Tiempo de respuesta del servicio. |
| `status_code` | `INTEGER` | C√≥digo HTTP de respuesta (ej. 200, 500). |
| `error_message` | `TEXT` | Detalle del error (Timeout, Connection Refused). |

## üîå API del Dashboard (Backend)

El backend del dashboard expone una API REST optimizada para consumo de m√©tricas hist√≥ricas y en tiempo real.

### `GET /api/live`

Retorna el estado actual del sistema y las series de tiempo hist√≥ricas.

* **Par√°metros:**
  * `range` (Query, opcional): Ventana de tiempo. Opciones: `live` (5m), `1h`, `12h`, `24h`, `7d`, `30d`. Default: `1h`.

* **Optimizaci√≥n (Resoluci√≥n Din√°mica):**
  El backend aplica autom√°ticamente un algoritmo de *downsampling* basado en la constante `TARGET_DATA_POINTS = 30`.
  * Si pides `24h`, la API agrupar√° los datos en buckets de ~48 minutos.
  * Si pides `live` (5m), los buckets ser√°n de 10 segundos (raw data).
  * **Beneficio:** El frontend siempre recibe ~30 puntos, manteniendo la renderizaci√≥n r√°pida y ligera.

* **M√©tricas Incluidas:**
  * **Jitter:** Calculado como `MAX(latency) - MIN(latency)` por bucket.
  * **Uptime %:** Calculado sobre el total de ciclos en el rango.
  * **Distribuci√≥n de Errores:** Conteo agrupado por c√≥digos de estado.

## ‚öôÔ∏è Configuraci√≥n y Variables de Entorno

El comportamiento del sistema se controla centralizadamente a trav√©s de variables de entorno (archivos `.env`).

### Credenciales y Endpoints

| Variable | Requerida | Descripci√≥n | Ejemplo |
| :--- | :---: | :--- | :--- |
| `SECRET_KEY` | **S√≠** | Clave compartida para autenticar con el Worker de Cloudflare. | `sk_12345abcdef` |
| `HEARTBEAT_URL` | **S√≠** | URL del endpoint del Cloudflare Worker para recibir latidos. | `https://worker.dev/api/heartbeat` |
| `N8N_WEBHOOK_URL` | No | URL del webhook para alertas externas. | `https://n8n.mi-server.com/...` |
| `SQLITE_DB_PATH` | No | Ruta interna para el archivo de base de datos. | `data/metrics.db` |

### Monitorizaci√≥n de Servicios

| Variable | Descripci√≥n | Ejemplo |
| :--- | :--- | :--- |
| `SERVICE_NAMES` | Lista separada por comas de identificadores de servicios. | `api,webapp,db_primary` |
| `SERVICE_URL_{NAME}` | URL de destino para el health check. Soporta `http(s)://` y `docker:`. | `docker:postgres-container` |
| `SERVICE_HEADERS_{NAME}`| Headers HTTP opcionales (Auth, User-Agent, etc.). | `Authorization:Bearer xyz` |

### Red Avanzada

| Variable | Descripci√≥n | Ejemplo |
| :--- | :--- | :--- |
| `INTERNAL_DNS_OVERRIDE_IP` | IP para forzar resoluci√≥n DNS local. √ötil para saltar NAT/Loopback. | `172.17.0.1` (Gateway Docker) |

### Configuraci√≥n Operacional (Avanzado)

| Variable | Descripci√≥n | Defecto |
| :--- | :--- | :--- |
| `LOOP_INTERVAL_SECONDS` | Intervalo del bucle principal del agente (en segundos). | `10` |
| `STATUS_CHANGE_THRESHOLD` | Umbral de confirmaci√≥n para cambios de estado (Debounce). | `4` |
| `SERVICE_TIMEOUT_SECONDS` | Tiempo de espera m√°ximo para cada health check. | `2` |
| `TARGET_DATA_POINTS` | Densidad de puntos en las gr√°ficas del dashboard (Bucketing). | `30` |
| `TZ` | Zona horaria del sistema (ej. `America/Lima`). | `UTC` |

## üõ†Ô∏è Configuraci√≥n y Despliegue

### Entorno de Producci√≥n

1. **Clonar el repositorio:**

    ```bash
    git clone https://github.com/iamseb4s/heartbeat-monitor.git
    cd heartbeat-monitor
    ```

2. **Configurar Variables:**
    * Copia `.env.prod.example` a `.env.prod`.
    * Rellena `SECRET_KEY`, `HEARTBEAT_URL`, `N8N_WEBHOOK_URL`, `SERVICE_NAMES` y las `SERVICE_URL_*` correspondientes.
3. **Ejecutar:**

    ```bash
    docker compose -f docker-compose.prod.yml up -d --build
    ```

4. **Acceso:**
    * **Dashboard:** `http://localhost:8100` (o la IP/dominio configurado).
    * **Logs del Agente:** `docker logs -f heartbeat-agent-prod`

### Entorno de Desarrollo (Local + Mock)

Para desarrollar sin afectar la base de datos de producci√≥n ni saturar el Worker real, utiliza el entorno aislado que incluye un **Mock Server**:

1. **Configurar Variables:** Copia `.env.dev.example` a `.env.dev`.
2. **Ejecutar:** `docker compose -f docker-compose.dev.yml up --build`
3. **Herramientas Disponibles:**
    * **Dashboard:** **<http://localhost:8098>** - Visualizaci√≥n de m√©tricas en tiempo real.
    * **Mock Controller:** **<http://localhost:8099>** - Simular ca√≠das, ver logs y forzar respuestas.

## üß™ Pruebas (Testing)

El proyecto incluye una suite completa de pruebas unitarias y de integraci√≥n para garantizar la fiabilidad de la l√≥gica de alertas, red y monitoreo.

* **Ejecuci√≥n Manual:** Ejecuta los tests dentro del contenedor de desarrollo:

  ```bash
  docker exec heartbeat-agent-dev pytest
  ```

* **Automatizaci√≥n (Git Hook):** Para ejecutar tests autom√°ticamente antes de cada merge, activa el hook incluido:

  ```bash
  git config core.hooksPath .githooks
  ```
